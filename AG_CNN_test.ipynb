{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zafer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zafer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\zafer\\AppData\\Local\\Temp\\ipykernel_16480\\1103524381.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(CKPT_PATH, map_location=\"cuda:0\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n",
      "=> Checkpoint loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 63/63 [00:36<00:00,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average AUROC is 0.557\n",
      "The AUROC of Emphysema is 0.451\n",
      "The AUROC of Cardiomegaly is 0.507\n",
      "The AUROC of Edema is 0.656\n",
      "The AUROC of Effusion is 0.623\n",
      "The AUROC of Atelectasis is 0.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# encoding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Fixed Evaluation Code for CheXNet\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import densenet121\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from read_data import ChestXrayDataSet\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "\n",
    "# Constants\n",
    "CKPT_PATH = 'best_model/5finding_trained_densenet121.pth.tar'  # Path to the trained model\n",
    "N_CLASSES = 5\n",
    "#CLASS_NAMES = [\n",
    "#    'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass',\n",
    "#    'Nodule', 'Pneumonia', 'Pneumothorax', 'Consolidation', 'Edema',\n",
    "#    'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia'\n",
    "#]\n",
    "CLASS_NAMES = [\"Emphysema\", \"Cardiomegaly\", \"Edema\", \"Effusion\", \"Atelectasis\"]\n",
    "\n",
    "DATA_DIR = r\"C:/Users/zafer/OneDrive/Masaüstü/224NIH/dataset/images-224/images-224\"\n",
    "TEST_IMAGE_LIST = 'labels/test_list_balanced.txt'\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "def tensor_transform(crops):\n",
    "    return torch.stack([transforms.ToTensor()(crop) for crop in crops])\n",
    "\n",
    "\n",
    "def normalize_transform(crops):\n",
    "    return torch.stack([transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(crop) for crop in crops])\n",
    "\n",
    "\n",
    "def create_transforms():\n",
    "    \"\"\"Create transformations for preprocessing.\"\"\"\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.TenCrop(224),\n",
    "        transforms.Lambda(tensor_transform),\n",
    "        transforms.Lambda(normalize_transform)\n",
    "    ])\n",
    "\n",
    "\n",
    "class DenseNet121(nn.Module):\n",
    "    \"\"\"DenseNet121 model for multi-label classification.\"\"\"\n",
    "    def __init__(self, out_size):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        self.densenet121 = densenet121(pretrained=False)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Linear(num_ftrs, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.densenet121(x)  # Raw logits output\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Initialize and load the trained DenseNet121 model.\"\"\"\n",
    "    model = DenseNet121(N_CLASSES)\n",
    "    if os.path.isfile(CKPT_PATH):\n",
    "        print(\"=> Loading checkpoint\")\n",
    "        checkpoint = torch.load(CKPT_PATH, map_location=\"cuda:0\")\n",
    "\n",
    "        if 'state_dict' in checkpoint:\n",
    "            state_dict = checkpoint['state_dict']\n",
    "            # Remove 'module.' prefix if saved with DataParallel\n",
    "            state_dict = {k.replace('module.', '')\n",
    "                          .replace(\"norm.1\",\"norm1\")\n",
    "                          .replace(\"conv.1\",\"conv1\")\n",
    "                          .replace(\"norm.2\",\"norm2\")\n",
    "                          .replace(\"conv.2\",\"conv2\")\n",
    "                          .replace(\"classifier.0\",\"classifier\"): v for k, v in state_dict.items()}\n",
    "            model.load_state_dict(state_dict)\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "\n",
    "        print(\"=> Checkpoint loaded\")\n",
    "    else:\n",
    "        print(\"=> No checkpoint found at the specified path\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Evaluate the model and compute AUROC.\"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    gt, pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inp, target in tqdm(test_loader, desc=\"Evaluating\", leave=True):\n",
    "            target = target.cuda()\n",
    "            bs, n_crops, c, h, w = inp.size()\n",
    "            inp = inp.view(-1, c, h, w).cuda()\n",
    "\n",
    "            # Get raw logits and apply sigmoid\n",
    "            logits = model(inp)\n",
    "            output_mean = torch.sigmoid(logits.view(bs, n_crops, -1).mean(1))\n",
    "\n",
    "            gt.append(target.cpu())\n",
    "            pred.append(output_mean.cpu())\n",
    "\n",
    "    gt = torch.cat(gt)\n",
    "    pred = torch.cat(pred)\n",
    "    return compute_AUCs(gt, pred)\n",
    "\n",
    "\n",
    "def compute_AUCs(gt, pred):\n",
    "    \"\"\"Compute Area Under the Curve (AUC) for all classes.\"\"\"\n",
    "    gt_np = gt.numpy()\n",
    "    pred_np = pred.numpy()\n",
    "    AUROCs = [roc_auc_score(gt_np[:, i], pred_np[:, i]) for i in range(N_CLASSES)]\n",
    "    return AUROCs\n",
    "\n",
    "\n",
    "def main():\n",
    "    cudnn.benchmark = True  # Enable optimized GPU performance\n",
    "\n",
    "    # Load model\n",
    "    model = load_model()\n",
    "    model = model.cuda()\n",
    "\n",
    "    # Prepare data\n",
    "    transform = create_transforms()\n",
    "    test_dataset = ChestXrayDataSet(data_dir=DATA_DIR, image_list_file=TEST_IMAGE_LIST, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # Evaluate\n",
    "    AUROCs = evaluate_model(model, test_loader)\n",
    "    AUROC_avg = np.mean(AUROCs)\n",
    "\n",
    "    print(f'The average AUROC is {AUROC_avg:.3f}')\n",
    "    for i, auroc in enumerate(AUROCs):\n",
    "        print(f'The AUROC of {CLASS_NAMES[i]} is {auroc:.3f}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Define a Function for Confusion Matrices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def compute_confusion_matrices(gt, pred, threshold=0.5):\n",
    "    \"\"\"Compute confusion matrices for each class.\"\"\"\n",
    "    gt_np = gt.numpy()\n",
    "    pred_np = (pred.numpy() > threshold).astype(int)  # Apply threshold to predictions\n",
    "\n",
    "    confusion_matrices = []\n",
    "    for i in range(N_CLASSES):\n",
    "        cm = confusion_matrix(gt_np[:, i], pred_np[:, i])\n",
    "        confusion_matrices.append(cm)\n",
    "\n",
    "    return confusion_matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Visualize Confusion Matrices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_name):\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "    plt.title(f'Confusion Matrix for {class_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zafer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zafer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\zafer\\AppData\\Local\\Temp\\ipykernel_16480\\1103524381.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(CKPT_PATH, map_location=\"cuda:0\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n",
      "=> Checkpoint loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 63/63 [00:29<00:00,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Emphysema:\n",
      "[[956   0]\n",
      " [ 44   0]]\n",
      "\n",
      "Confusion Matrix for Cardiomegaly:\n",
      "[[942   0]\n",
      " [ 58   0]]\n",
      "\n",
      "Confusion Matrix for Edema:\n",
      "[[968   0]\n",
      " [ 32   0]]\n",
      "\n",
      "Confusion Matrix for Effusion:\n",
      "[[737   2]\n",
      " [261   0]]\n",
      "\n",
      "Confusion Matrix for Atelectasis:\n",
      "[[761   0]\n",
      " [239   0]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = load_model()\n",
    "model = model.cuda()\n",
    "\n",
    "# Prepare data\n",
    "transform = create_transforms()\n",
    "test_dataset = ChestXrayDataSet(data_dir=DATA_DIR, image_list_file=TEST_IMAGE_LIST, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Evaluate to get gt and pred\n",
    "gt, pred = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inp, target in tqdm(test_loader, desc=\"Evaluating\", leave=True):\n",
    "        target = target.cuda()\n",
    "        bs, n_crops, c, h, w = inp.size()\n",
    "        inp = inp.view(-1, c, h, w).cuda()\n",
    "        logits = model(inp)\n",
    "        output_mean = torch.sigmoid(logits.view(bs, n_crops, -1).mean(1))\n",
    "        gt.append(target.cpu())\n",
    "        pred.append(output_mean.cpu())\n",
    "gt = torch.cat(gt)\n",
    "pred = torch.cat(pred)\n",
    "\n",
    "# Compute confusion matrices\n",
    "# Compute confusion matrices\n",
    "confusion_matrices = compute_confusion_matrices(gt, pred)\n",
    "\n",
    "# Print confusion matrices\n",
    "for i, cm in enumerate(confusion_matrices):\n",
    "    print(f'Confusion Matrix for {CLASS_NAMES[i]}:\\n{cm}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Precision, Recall, and F1 Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_precision_recall_f1(gt, pred, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and F1 score for each class.\n",
    "    \n",
    "    Parameters:\n",
    "    - gt: Tensor of ground truth labels (shape: [num_samples, num_classes]).\n",
    "    - pred: Tensor of predicted probabilities (shape: [num_samples, num_classes]).\n",
    "    - threshold: Threshold for converting probabilities to binary predictions.\n",
    "    \n",
    "    Returns:\n",
    "    - precisions: List of precision scores for each class.\n",
    "    - recalls: List of recall scores for each class.\n",
    "    - f1_scores: List of F1 scores for each class.\n",
    "    \"\"\"\n",
    "    # Convert tensors to numpy arrays\n",
    "    gt_np = gt.cpu().numpy()\n",
    "    pred_np = (pred.cpu().numpy() > threshold).astype(int)  # Apply threshold to predictions\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Loop over each class and calculate metrics\n",
    "    num_classes = gt_np.shape[1]\n",
    "    for i in range(num_classes):\n",
    "        precisions.append(precision_score(gt_np[:, i], pred_np[:, i], zero_division=0))\n",
    "        recalls.append(recall_score(gt_np[:, i], pred_np[:, i], zero_division=0))\n",
    "        f1_scores.append(f1_score(gt_np[:, i], pred_np[:, i], zero_division=0))\n",
    "\n",
    "    return precisions, recalls, f1_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Class-Wise Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emphysema: Precision=0.0000, Recall=0.0000, F1-Score=0.0000\n",
      "Cardiomegaly: Precision=0.0000, Recall=0.0000, F1-Score=0.0000\n",
      "Edema: Precision=0.0000, Recall=0.0000, F1-Score=0.0000\n",
      "Effusion: Precision=0.3391, Recall=0.6054, F1-Score=0.4347\n",
      "Atelectasis: Precision=0.2500, Recall=0.4854, F1-Score=0.3300\n"
     ]
    }
   ],
   "source": [
    "# Assuming gt and pred are tensors of shape [num_samples, num_classes]\n",
    "precisions, recalls, f1_scores = compute_precision_recall_f1(gt, pred, threshold=0.3)\n",
    "\n",
    "# Print the metrics\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    print(f\"{class_name}: Precision={precisions[i]:.4f}, Recall={recalls[i]:.4f}, F1-Score={f1_scores[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhgAAAIjCAYAAABBOWJ+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/GU6VOAAAACXBIWXMAAA9hAAAPYQGoP6dpAABWDElEQVR4nO3dd3RU1d7G8WcSkklCCAFDQgtEepUShBsQKdIR5VpARdqliMBViQ1UiNgABcSrFOUqeBWliRUEQlNBFASChV6DSEJRCBBIPe8fvBkZZhJSdhgC389arMXsU2bPb8p5Zp99JjbLsiwBAAAY5OXpDgAAgGsPAQMAABhHwAAAAMYRMAAAgHEEDAAAYBwBAwAAGEfAAAAAxhEwAACAcQQMAABgHAEDV5V+/fopIiIiT9usWbNGNptNa9asKZQ+FXWtW7dW69atHbcPHDggm82m2bNne6xPnnbmzBkNHDhQZcuWlc1m02OPPebpLl0RvBYu4DPjyiBgXOdmz54tm83m+Ofn56caNWpo+PDhSkxM9HT3rnpZH9BZ/7y8vFS6dGl17txZ69ev93T3jEhMTNQTTzyhWrVqKSAgQMWLF1dkZKReeuklnTx50tPdy5dXXnlFs2fP1sMPP6wPPvhAvXv3LvT7zMjI0KxZs9S6dWuVLl1adrtdERER6t+/v3766adCv3/gSivm6Q7g6vDCCy/oxhtv1Pnz57V27VpNnz5dS5Ys0a+//qqAgIAr1o+ZM2cqMzMzT9vceuutOnfunHx9fQupV5d3//33q0uXLsrIyNCuXbs0bdo0tWnTRhs3blT9+vU91q+C2rhxo7p06aIzZ87owQcfVGRkpCTpp59+0vjx4/Xtt99q+fLlHu5l3q1atUr/+Mc/FBMTc0Xu79y5c7rrrru0dOlS3XrrrXrmmWdUunRpHThwQPPnz9f777+v+Ph4VaxY8Yr0J0vlypV17tw5+fj4XNH7xfWBgAFJUufOndWkSRNJ0sCBA3XDDTdo8uTJ+vzzz3X//fe73ebs2bMqXry40X7k54POy8tLfn5+RvuRV40bN9aDDz7ouN2yZUt17txZ06dP17Rp0zzYs/w7efKk/vnPf8rb21tbtmxRrVq1nJa//PLLmjlzppH7KozXUk6OHj2qOnXqGNtfenq6MjMzsw25Tz75pJYuXarXX3/d5XRMTEyMXn/9dSP9yGsds0YtgcLAKRK41bZtW0nS/v37JV2YGxEYGKi9e/eqS5cuKlGihHr16iVJyszM1JQpU1S3bl35+fkpLCxMDz30kP766y+X/X799ddq1aqVSpQooaCgIN1888366KOPHMvdzcGYO3euIiMjHdvUr19fb7zxhmN5dudTFyxYoMjISPn7+yskJEQPPvigDh8+7LRO1uM6fPiwunfvrsDAQJUpU0ZPPPGEMjIy8l2/li1bSpL27t3r1H7y5Ek99thjCg8Pl91uV7Vq1TRhwgSXUZvMzEy98cYbql+/vvz8/FSmTBl16tTJaSh91qxZatu2rUJDQ2W321WnTh1Nnz49332+1Ntvv63Dhw9r8uTJLuFCksLCwvTcc885bttsNj3//PMu60VERKhfv36O21mn5b755hsNHTpUoaGhqlixohYuXOhod9cXm82mX3/91dG2Y8cO3XPPPSpdurT8/PzUpEkTffHFFzk+pqzXyv79+7V48WLHqa0DBw5IuhA8BgwYoLCwMPn5+alBgwZ6//33nfaRdVps4sSJmjJliqpWrSq73a5t27a5vc/ff/9db7/9ttq3b+92roe3t7eeeOIJx+jFwYMHNXToUNWsWVP+/v664YYbdO+99zr6eLk6ZnnnnXdUtWpV+fv7q2nTpvruu+9c7ju7ORirVq1Sy5YtVbx4cQUHB+vOO+/U9u3bndZ5/vnnZbPZtGvXLj344IMqWbKkypQpo9GjR8uyLB06dEh33nmngoKCVLZsWU2aNMnl/lNSUhQTE6Nq1arJbrcrPDxcTz31lFJSUpzWO3funB555BGFhISoRIkSuuOOO3T48GGX11xua3epmJgY+fj46NixYy7LBg8erODgYJ0/fz7HfcAVIxhwK+vAeMMNNzja0tPT1bFjR91yyy2aOHGi49TJQw89pNmzZ6t///565JFHtH//fr311lvasmWL1q1b5xiVmD17tv71r3+pbt26GjVqlIKDg7VlyxYtXbpUDzzwgNt+xMbG6v7779dtt92mCRMmSJK2b9+udevW6dFHH822/1n9ufnmmzVu3DglJibqjTfe0Lp167RlyxYFBwc71s3IyFDHjh3VrFkzTZw4UStWrNCkSZNUtWpVPfzww/mqX9YHWqlSpRxtycnJatWqlQ4fPqyHHnpIlSpV0vfff69Ro0bpyJEjmjJlimPdAQMGaPbs2ercubMGDhyo9PR0fffdd/rhhx8cI03Tp09X3bp1dccdd6hYsWL68ssvNXToUGVmZmrYsGH56vfFvvjiC/n7++uee+4p8L7cGTp0qMqUKaMxY8bo7Nmz6tq1qwIDAzV//ny1atXKad158+apbt26qlevniTpt99+U4sWLVShQgWNHDlSxYsX1/z589W9e3d98skn+uc//+n2PmvXrq0PPvhAI0aMUMWKFfX4449LksqUKaNz586pdevW2rNnj4YPH64bb7xRCxYsUL9+/XTy5EmX19usWbN0/vx5DR48WHa7XaVLl3Z7n19//bXS09NzPc9j48aN+v7773XfffepYsWKOnDggKZPn67WrVtr27ZtLqcsL62jJL377rt66KGH1Lx5cz322GPat2+f7rjjDpUuXVrh4eE53v+KFSvUuXNnValSRc8//7zOnTunN998Uy1atNDmzZtdvgD07NlTtWvX1vjx47V48WK99NJLKl26tN5++221bdtWEyZM0Jw5c/TEE0/o5ptv1q233irpQoi+4447tHbtWg0ePFi1a9fWL7/8otdff127du3SZ5995riPfv36af78+erdu7f+8Y9/6JtvvlHXrl0LXLssvXv31gsvvKB58+Zp+PDhjvbU1FQtXLhQd999NyM9+WHhujZr1ixLkrVixQrr2LFj1qFDh6y5c+daN9xwg+Xv72/9/vvvlmVZVt++fS1J1siRI522/+677yxJ1pw5c5zaly5d6tR+8uRJq0SJElazZs2sc+fOOa2bmZnp+H/fvn2typUrO24/+uijVlBQkJWenp7tY1i9erUlyVq9erVlWZaVmppqhYaGWvXq1XO6r6+++sqSZI0ZM8bp/iRZL7zwgtM+GzVqZEVGRmZ7n1n2799vSbLGjh1rHTt2zEpISLC+++476+abb7YkWQsWLHCs++KLL1rFixe3du3a5bSPkSNHWt7e3lZ8fLxlWZa1atUqS5L1yCOPuNzfxbVKTk52Wd6xY0erSpUqTm2tWrWyWrVq5dLnWbNm5fjYSpUqZTVo0CDHdS4myYqJiXFpr1y5stW3b1/H7azX3C233OLyvN5///1WaGioU/uRI0csLy8vp+fotttus+rXr2+dP3/e0ZaZmWk1b97cql69+mX7WrlyZatr165ObVOmTLEkWR9++KGjLTU11YqKirICAwOtpKQky7L+rl9QUJB19OjRy97XiBEjLEnWli1bLruuZbl/XtevX29Jsv73v/852rKrY9brv2HDhlZKSoqj/Z133rEkXfa10LBhQys0NNQ6ceKEo23r1q2Wl5eX1adPH0dbTEyMJckaPHiwoy09Pd2qWLGiZbPZrPHjxzva//rrL8vf39/pdfDBBx9YXl5e1nfffef0WGfMmGFJstatW2dZlmVt2rTJkmQ99thjTuv169fP5TWX29pd+plhWZYVFRVlNWvWzGnbRYsWuayH3OMUCSRJ7dq1U5kyZRQeHq777rtPgYGB+vTTT1WhQgWn9S79Rr9gwQKVLFlS7du31/Hjxx3/IiMjFRgYqNWrV0u6MBJx+vRpjRw50uWbgM1my7ZfwcHBOnv2rGJjY3P9WH766ScdPXpUQ4cOdbqvrl27qlatWlq8eLHLNkOGDHG63bJlS+3bty/X9xkTE6MyZcqobNmyatmypbZv365JkyY5fftfsGCBWrZsqVKlSjnVql27dsrIyNC3334rSfrkk09ks9ncTkC8uFb+/v6O/586dUrHjx9Xq1attG/fPp06dSrXfc9OUlKSSpQoUeD9ZGfQoEHy9vZ2auvZs6eOHj3qdLpr4cKFyszMVM+ePSVJf/75p1atWqUePXro9OnTjjqeOHFCHTt21O7du11OheXGkiVLVLZsWac5Rz4+PnrkkUd05swZl1M3d999t8qUKXPZ/SYlJUlSrmt58fOalpamEydOqFq1agoODtbmzZtd1r+0jlmv/yFDhjjNCenXr59KliyZ430fOXJEcXFx6tevn9OIzE033aT27dtryZIlLtsMHDjQ8X9vb281adJElmVpwIABjvbg4GDVrFnT6T21YMEC1a5dW7Vq1XJ6P2Sdns367Fi6dKmkCyM1F/v3v//t0pe81u5iffr00Y8//uh0WnPOnDkKDw93GVFD7nCKBJKkqVOnqkaNGipWrJjCwsJUs2ZNeXk5589ixYq5zHLfvXu3Tp06pdDQULf7PXr0qKS/T7lkDXHn1tChQzV//nx17txZFSpUUIcOHdSjRw916tQp220OHjwoSapZs6bLslq1amnt2rVObVlzHC5WqlQppzkkx44dc5qTERgYqMDAQMftwYMH695779X58+e1atUq/ec//3GZw7F79279/PPP2R6ULq5V+fLlsx1yz7Ju3TrFxMRo/fr1Sk5Odlp26tSpyx5MLicoKEinT58u0D5ycuONN7q0derUSSVLltS8efN02223SbpweqRhw4aqUaOGJGnPnj2yLEujR4/W6NGj3e776NGjLuH4cg4ePKjq1au7vO5r167tWH65/rsTFBQkSbmu5blz5zRu3DjNmjVLhw8flmVZjmXuguOl/cjqZ/Xq1Z3afXx8VKVKlRzvO6f3Tu3atbVs2TKXiaSVKlVyWq9kyZLy8/NTSEiIS/uJEycct3fv3q3t27df9v1w8OBBeXl5uTzOatWquWyT19pdrGfPnnrsscc0Z84cjRkzRqdOndJXX32lESNG5PglCNkjYECS1LRpU8e5/ezY7XaXD9/MzEyFhoZqzpw5brfJzTe8nISGhiouLk7Lli3T119/ra+//lqzZs1Snz59XCbf5del36Ldufnmm50OMDExMU6Ty6pXr6527dpJkm6//XZ5e3tr5MiRatOmjaOumZmZat++vZ566im395F1AM2NvXv36rbbblOtWrU0efJkhYeHy9fXV0uWLNHrr7+e50t93alVq5bi4uKUmppaoEuAs5sse/G3zSx2u13du3fXp59+qmnTpikxMVHr1q3TK6+84lgn67E98cQT6tixo9t9uzv4mOau/+5kTZD95Zdf1LBhw8uu/+9//1uzZs3SY489pqioKJUsWVI2m0333Xef2+c1t/0oLO7eP9m9py4+4GdmZqp+/fqaPHmy23UvN1fEnbzW7mKlSpXS7bff7ggYCxcuVEpKitPVYcgbAgYKpGrVqlqxYoVatGiR4wdd1apVJUm//vprnj/8fX191a1bN3Xr1k2ZmZkaOnSo3n77bY0ePdrtvipXrixJ2rlzp2O4NcvOnTsdy/Nizpw5OnfunOP25b4JPvvss5o5c6aee+45xxBv1apVdebMGUcQyU7VqlW1bNky/fnnn9mOYnz55ZdKSUnRF1984fQNMmtY2YRu3bpp/fr1+uSTT7K9VPlipUqVcvnhrdTUVB05ciRP99uzZ0+9//77WrlypbZv3y7LshynR6S/a+/j43PZWuZF5cqV9fPPPyszM9MpSO/YscOxPD86d+4sb29vffjhh7ma6Llw4UL17dvX6aqL8+fP5/pHzbL6uXv3bqfXf1pamvbv368GDRpcdtudO3e6LNuxY4dCQkKMXU5ctWpVbd26VbfddluOIwSVK1dWZmam9u/f7zQqs2fPHpd1C1q7Pn366M4779TGjRs1Z84cNWrUSHXr1s39g4IT5mCgQHr06KGMjAy9+OKLLsvS09Mdb+wOHTqoRIkSGjdunMvlXhd/q7nUxUOq0oXfvLjpppskyeVStixNmjRRaGioZsyY4bTO119/re3bt7udfX45LVq0ULt27Rz/LhcwgoOD9dBDD2nZsmWKi4uTdKFW69ev17Jly1zWP3nypNLT0yVdOLdvWZbGjh3rsl5WrbK+IV46BDxr1qw8P7bsDBkyROXKldPjjz+uXbt2uSw/evSoXnrpJcftqlWrOuaRZHnnnXfyfLlvu3btVLp0ac2bN0/z5s1T06ZNnYbHQ0ND1bp1a7399ttuw4u7Sw1zo0uXLkpISNC8efMcbenp6XrzzTcVGBiY7/Pw4eHhGjRokJYvX64333zTZXlmZqYmTZqk33//XdKF5/bS98Sbb76Z6zo2adJEZcqU0YwZM5Samuponz179mUPtOXKlVPDhg31/vvvO63766+/avny5erSpUuu+pAbPXr00OHDh93+lsq5c+ccV8RkjVJd+nsy7mpZ0Np17txZISEhmjBhgr755htGLwqIEQwUSKtWrfTQQw9p3LhxiouLU4cOHeTj46Pdu3drwYIFeuONN3TPPfcoKChIr7/+ugYOHKibb75ZDzzwgEqVKqWtW7cqOTk529MdAwcO1J9//qm2bduqYsWKOnjwoN588001bNjQcW78Uj4+PpowYYL69++vVq1a6f7773dcphoREaERI0YUZkkcHn30UU2ZMkXjx4/X3Llz9eSTT+qLL77Q7bffrn79+ikyMlJnz57VL7/8ooULF+rAgQMKCQlRmzZt1Lt3b/3nP//R7t271alTJ2VmZuq7775TmzZtNHz4cHXo0MExsvPQQw/pzJkzmjlzpkJDQ/M8YpCdUqVK6dNPP1WXLl3UsGFDp1/y3Lx5sz7++GNFRUU51h84cKCGDBmiu+++W+3bt9fWrVu1bNkyl3Pxl+Pj46O77rpLc+fO1dmzZzVx4kSXdaZOnapbbrlF9evX16BBg1SlShUlJiZq/fr1+v3337V169Y8P97Bgwfr7bffVr9+/bRp0yZFRERo4cKFWrdunaZMmVKgCa+TJk3S3r179cgjj2jRokW6/fbbVapUKcXHx2vBggXasWOH7rvvPkkXTrF98MEHKlmypOrUqaP169drxYoVTpeM58THx0cvvfSSHnroIbVt21Y9e/bU/v37NWvWrMsGY0l67bXX1LlzZ0VFRWnAgAGOy1RLlizp9ndO8qt3796aP3++hgwZotWrV6tFixbKyMjQjh07NH/+fC1btkxNmjRRZGSk7r77bk2ZMkUnTpxwXKaaFXovHv0wUbv77rtPb731lry9vXM1cocceObiFVwtsi5127hxY47r9e3b1ypevHi2y9955x0rMjLS8vf3t0qUKGHVr1/feuqpp6w//vjDab0vvvjCat68ueXv728FBQVZTZs2tT7++GOn+7n4MtWFCxdaHTp0sEJDQy1fX1+rUqVK1kMPPWQdOXLEsY67S84sy7LmzZtnNWrUyLLb7Vbp0qWtXr16OS67vdzjyroE73KyLvN77bXX3C7v16+f5e3tbe3Zs8eyLMs6ffq0NWrUKKtatWqWr6+vFRISYjVv3tyaOHGilZqa6tguPT3deu2116xatWpZvr6+VpkyZazOnTtbmzZtcqrlTTfdZPn5+VkRERHWhAkTrPfee8+SZO3fv9+xXn4vU83yxx9/WCNGjLBq1Khh+fn5WQEBAVZkZKT18ssvW6dOnXKsl5GRYT399NNWSEiIFRAQYHXs2NHas2dPtpep5vSai42NtSRZNpvNOnTokNt19u7da/Xp08cqW7as5ePjY1WoUMG6/fbbrYULF172Mbm7TNWyLCsxMdHq37+/FRISYvn6+lr169d3qdPlnvPspKenW//973+tli1bWiVLlrR8fHysypUrW/3793e6hPWvv/5y9CEwMNDq2LGjtWPHjjzXcdq0adaNN95o2e12q0mTJta3336b69fCihUrrBYtWjjep926dbO2bdvmtE7We+TYsWNO7dm9p1q1amXVrVvXqS01NdWaMGGCVbduXctut1ulSpWyIiMjrbFjxzq9ts6ePWsNGzbMKl26tBUYGGh1797d2rlzpyXJ5XLY3NQuu88My7KsDRs2WJKsDh06uK0rcs9mWTmMTwMAcBWKi4tTo0aN9OGHHzp+VdiErVu3qmHDhvrf//53Rf4I3rWMORgAgKvaxROss0yZMkVeXl6OXwY1ZebMmQoMDNRdd91ldL/XI+ZgAACuaq+++qo2bdqkNm3aqFixYo5L1gcPHpyvy1nd+fLLL7Vt2za98847Gj58+BX943vXKk6RAACuarGxsRo7dqy2bdumM2fOqFKlSurdu7eeffZZFStm5ntyRESEEhMT1bFjR33wwQeF+iu21wsCBgAAMI45GAAAwDgCBgAAMO66m+SZmZmpP/74QyVKlOAP2AAAkAeWZen06dMqX768y9+mutR1FzD++OMPY7OOAQC4Hh06dMjlr2tf6roLGFkzgw8dOuT4M8oFlZaWpuXLlzt+JhsFR03No6ZmUU/zqKlZhVHPpKQkhYeH5+oqm+suYGSdFgkKCjIaMAICAhQUFMSbwhBqah41NYt6mkdNzSrMeuZmigGTPAEAgHEEDAAAYBwBAwAAGEfAAAAAxhEwAACAcQQMAABgHAEDAAAYR8AAAADGETAAAIBxBAwAAGAcAQMAABhHwAAAAMYRMAAAgHEEDAAAYJxHA8a3336rbt26qXz58rLZbPrss88uu82aNWvUuHFj2e12VatWTbNnzy70fgIAgLzxaMA4e/asGjRooKlTp+Zq/f3796tr165q06aN4uLi9Nhjj2ngwIFatmxZIfc0Z0dOndfuUzYdOXXeo/0AAOBqUcyTd965c2d17tw51+vPmDFDN954oyZNmiRJql27ttauXavXX39dHTt2LKxu5mjexniNXPSLLMtb07Z/q3F31VfPmyt5pC8AAFwtPBow8mr9+vVq166dU1vHjh312GOPZbtNSkqKUlJSHLeTkpIkSWlpaUpLSytQf46cOq9Ri36RZV24nWlJoxb9oqgbS6lcSb8C7ft6l/XcFPQ5wt+oqVnU0zxqalZh1DMv+ypSASMhIUFhYWFObWFhYUpKStK5c+fk7+/vss24ceM0duxYl/bly5crICCgQP3ZfcqmTMvbqS3TkuYvWa3qJa0C7RsXxMbGeroL1xxqahb1NI+ammWynsnJyblet0gFjPwYNWqUoqOjHbeTkpIUHh6uDh06KCgoqED7PnLqvKZt/1aZF2UJL5vUo0sbRjAKKC0tTbGxsWrfvr18fHw83Z1rAjU1i3qaR03NKox6Zp0FyI0iFTDKli2rxMREp7bExEQFBQW5Hb2QJLvdLrvd7tLu4+NT4IJXCvHRuLvqa+Qnv8iSZJM07q76qhRSokD7xd9MPE9wRk3Nop7mUVOzTNYzL/spUr+DERUVpZUrVzq1xcbGKioqykM9knreXEkd64RKkoa0imCCJwAA8nDAOHPmjOLi4hQXFyfpwmWocXFxio+Pl3Th9EafPn0c6w8ZMkT79u3TU089pR07dmjatGmaP3++RowY4YnuO/j7XpiHUcKPxA0AgOThgPHTTz+pUaNGatSokSQpOjpajRo10pgxYyRJR44ccYQNSbrxxhu1ePFixcbGqkGDBpo0aZL++9//euwSVQAA4J5H52C0bt1alpX91RbufqWzdevW2rJlSyH2CgAAFFSRmoMBAACKBgIGAAAwjoABAACMI2AAAADjCBgAAMA4AgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjCBgAAMA4AgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjCBgAAMA4AgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjCBgAAMA4AgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjCBgAAMA4AgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjCBgAAMA4AgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAOI8HjKlTpyoiIkJ+fn5q1qyZNmzYkOP6U6ZMUc2aNeXv76/w8HCNGDFC58+fv0K9BQAAueHRgDFv3jxFR0crJiZGmzdvVoMGDdSxY0cdPXrU7fofffSRRo4cqZiYGG3fvl3vvvuu5s2bp2eeeeYK9xwAAOTEowFj8uTJGjRokPr37686depoxowZCggI0Hvvved2/e+//14tWrTQAw88oIiICHXo0EH333//ZUc9AADAlVXMU3ecmpqqTZs2adSoUY42Ly8vtWvXTuvXr3e7TfPmzfXhhx9qw4YNatq0qfbt26clS5aod+/e2d5PSkqKUlJSHLeTkpIkSWlpaUpLSzPyWDIzLUlSRkamsX1e77LqSD3NoaZmUU/zqKlZhVHPvOzLYwHj+PHjysjIUFhYmFN7WFiYduzY4XabBx54QMePH9ctt9wiy7KUnp6uIUOG5HiKZNy4cRo7dqxL+/LlyxUQEFCwB/H//jjiJclLu3ft0pKzO43sExfExsZ6ugvXHGpqFvU0j5qaZbKeycnJuV7XYwEjP9asWaNXXnlF06ZNU7NmzbRnzx49+uijevHFFzV69Gi324waNUrR0dGO20lJSQoPD1eHDh0UFBRkpF+rFvwsHUtQ9Ro11KVVVSP7vN6lpaUpNjZW7du3l4+Pj6e7c02gpmZRT/OoqVmFUc+sswC54bGAERISIm9vbyUmJjq1JyYmqmzZsm63GT16tHr37q2BAwdKkurXr6+zZ89q8ODBevbZZ+Xl5TqlxG63y263u7T7+PgYK7iXl02S5O3txZvCMJPPEy6gpmZRT/OoqVkm65mX/Xhskqevr68iIyO1cuVKR1tmZqZWrlypqKgot9skJye7hAhvb29JkmVZhddZAACQJx49RRIdHa2+ffuqSZMmatq0qaZMmaKzZ8+qf//+kqQ+ffqoQoUKGjdunCSpW7dumjx5sho1auQ4RTJ69Gh169bNETQAAIDneTRg9OzZU8eOHdOYMWOUkJCghg0baunSpY6Jn/Hx8U4jFs8995xsNpuee+45HT58WGXKlFG3bt308ssve+ohAAAANzw+yXP48OEaPny422Vr1qxxul2sWDHFxMQoJibmCvQMAADkl8d/KhwAAFx7CBgAAMA4AgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjCBgAAMA4AgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjCBgAAMA4AgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjCBgAAMA4AgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjCBgAAMA4AgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjCBgAAMA4AgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjPB4wpk6dqoiICPn5+alZs2basGFDjuufPHlSw4YNU7ly5WS321WjRg0tWbLkCvUWAADkRjFP3vm8efMUHR2tGTNmqFmzZpoyZYo6duyonTt3KjQ01GX91NRUtW/fXqGhoVq4cKEqVKiggwcPKjg4+Mp3HgAAZMujAWPy5MkaNGiQ+vfvL0maMWOGFi9erPfee08jR450Wf+9997Tn3/+qe+//14+Pj6SpIiIiCvZZQAAkAseCxipqanatGmTRo0a5Wjz8vJSu3bttH79erfbfPHFF4qKitKwYcP0+eefq0yZMnrggQf09NNPy9vb2+02KSkpSklJcdxOSkqSJKWlpSktLc3IY8nMtCRJGRmZxvZ5vcuqI/U0h5qaRT3No6ZmFUY987IvjwWM48ePKyMjQ2FhYU7tYWFh2rFjh9tt9u3bp1WrVqlXr15asmSJ9uzZo6FDhyotLU0xMTFutxk3bpzGjh3r0r58+XIFBAQU/IFI+uOIlyQv7d61S0vO7jSyT1wQGxvr6S5cc6ipWdTTPGpqlsl6Jicn53pdj54iyavMzEyFhobqnXfekbe3tyIjI3X48GG99tpr2QaMUaNGKTo62nE7KSlJ4eHh6tChg4KCgoz0a9WCn6VjCapeo4a6tKpqZJ/Xu7S0NMXGxqp9+/aO02EoGGpqFvU0j5qaVRj1zDoLkBseCxghISHy9vZWYmKiU3tiYqLKli3rdpty5crJx8fH6XRI7dq1lZCQoNTUVPn6+rpsY7fbZbfbXdp9fHyMFdzLyyZJ8vb24k1hmMnnCRdQU7Oop3nU1CyT9czLfjx2maqvr68iIyO1cuVKR1tmZqZWrlypqKgot9u0aNFCe/bsUWZmpqNt165dKleunNtwAQAAPMOjv4MRHR2tmTNn6v3339f27dv18MMP6+zZs46rSvr06eM0CfThhx/Wn3/+qUcffVS7du3S4sWL9corr2jYsGGeeggAAMANj87B6Nmzp44dO6YxY8YoISFBDRs21NKlSx0TP+Pj4+Xl9XcGCg8P17JlyzRixAjddNNNqlChgh599FE9/fTTnnoIAADADY9P8hw+fLiGDx/udtmaNWtc2qKiovTDDz8Ucq8AAEBBePynwgEAwLWHgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAuHz90FZGRoZmz56tlStX6ujRo05/G0SSVq1aZaRzAACgaMpXwHj00Uc1e/Zsde3aVfXq1ZPNZjPdLwAAUITlK2DMnTtX8+fPV5cuXUz3BwAAXAPyNQfD19dX1apVM90XAABwjchXwHj88cf1xhtvyLIs0/0BAADXgHydIlm7dq1Wr16tr7/+WnXr1pWPj4/T8kWLFhnpHAAAKJryFTCCg4P1z3/+03RfAADANSJfAWPWrFmm+wEAAK4h+QoYWY4dO6adO3dKkmrWrKkyZcoY6RQAACja8jXJ8+zZs/rXv/6lcuXK6dZbb9Wtt96q8uXLa8CAAUpOTjbdRwAAUMTkK2BER0frm2++0ZdffqmTJ0/q5MmT+vzzz/XNN9/o8ccfN91HAABQxOTrFMknn3yihQsXqnXr1o62Ll26yN/fXz169ND06dNN9Q8AABRB+RrBSE5OVlhYmEt7aGgop0gAAED+AkZUVJRiYmJ0/vx5R9u5c+c0duxYRUVFGescAAAomvJ1iuSNN95Qx44dVbFiRTVo0ECStHXrVvn5+WnZsmVGOwgAAIqefAWMevXqaffu3ZozZ4527NghSbr//vvVq1cv+fv7G+0gAAAoevL9OxgBAQEaNGiQyb4AAIBrRK4DxhdffKHOnTvLx8dHX3zxRY7r3nHHHQXuGAAAKLpyHTC6d++uhIQEhYaGqnv37tmuZ7PZlJGRYaJvAACgiMp1wMjMzHT7fwAAgEvl6zJVd06ePGlqVwAAoIjLV8CYMGGC5s2b57h97733qnTp0qpQoYK2bt1qrHMAAKBoylfAmDFjhsLDwyVJsbGxWrFihZYuXarOnTvrySefNNpBAABQ9OTrMtWEhARHwPjqq6/Uo0cPdejQQREREWrWrJnRDgIAgKInXyMYpUqV0qFDhyRJS5cuVbt27SRJlmVxBQkAAMjfCMZdd92lBx54QNWrV9eJEyfUuXNnSdKWLVtUrVo1ox0EAABFT74Cxuuvv66IiAgdOnRIr776qgIDAyVJR44c0dChQ412EAAAFD35Chg+Pj564oknXNpHjBhR4A4BAICij58KBwAAxvFT4QAAwDh+KhwAABhn7KfCAQAAsuQrYDzyyCP6z3/+49L+1ltv6bHHHitonwAAQBGXr4DxySefqEWLFi7tzZs318KFCwvcKQAAULTlK2CcOHFCJUuWdGkPCgrS8ePHC9wpAABQtOUrYFSrVk1Lly51af/6669VpUqVAncKAAAUbfn6oa3o6GgNHz5cx44dU9u2bSVJK1eu1KRJkzRlyhST/QMAAEVQvgLGv/71L6WkpOjll1/Wiy++KEmKiIjQ9OnT1adPH6MdBAAARU++AoYkPfzww3r44Yd17Ngx+fv7O/4eCQAAQL5/ByM9PV0rVqzQokWLZFmWJOmPP/7QmTNnjHUOAAAUTfkawTh48KA6deqk+Ph4paSkqH379ipRooQmTJiglJQUzZgxw3Q/AQBAEZKvEYxHH31UTZo00V9//SV/f39H+z//+U+tXLnSWOcAAEDRlK8RjO+++07ff/+9fH19ndojIiJ0+PBhIx0DAABFV75GMDIzM93+xdTff/9dJUqUKHCnAABA0ZavgNGhQwen37uw2Ww6c+aMYmJi1KVLF1N9AwAARVS+TpFMnDhRnTp1Up06dXT+/Hk98MAD2r17t0JCQvTxxx+b7iMAAChi8hUwwsPDtXXrVs2bN09bt27VmTNnNGDAAPXq1ctp0icAALg+5TlgpKWlqVatWvrqq6/Uq1cv9erVqzD6BQAAirA8z8Hw8fHR+fPnC6MvAADgGpGvSZ7Dhg3ThAkTlJ6ebro/AADgGpCvORgbN27UypUrtXz5ctWvX1/Fixd3Wr5o0SIjnQMAAEVTvgJGcHCw7r77btN9AQAA14g8BYzMzEy99tpr2rVrl1JTU9W2bVs9//zzXDkCAACc5GkOxssvv6xnnnlGgYGBqlChgv7zn/9o2LBhhdU3AABQROUpYPzvf//TtGnTtGzZMn322Wf68ssvNWfOHGVmZhZW/wAAQBGUp4ARHx/v9FPg7dq1k81m0x9//GG8YwAAoOjKU8BIT0+Xn5+fU5uPj4/S0tKMdgoAABRteZrkaVmW+vXrJ7vd7mg7f/68hgwZ4nSpKpepAgBwfctTwOjbt69L24MPPmisMwAA4NqQp4Axa9aswuoHAAC4huTrp8IBAAByQsAAAADGETAAAIBxBAwAAGAcAQMAABh3VQSMqVOnKiIiQn5+fmrWrJk2bNiQq+3mzp0rm82m7t27F24HAQBAnng8YMybN0/R0dGKiYnR5s2b1aBBA3Xs2FFHjx7NcbsDBw7oiSeeUMuWLa9QTwEAQG55PGBMnjxZgwYNUv/+/VWnTh3NmDFDAQEBeu+997LdJiMjQ7169dLYsWNVpUqVK9hbAACQG3n6oS3TUlNTtWnTJo0aNcrR5uXlpXbt2mn9+vXZbvfCCy8oNDRUAwYM0HfffZfjfaSkpCglJcVxOykpSZKUlpZm7G+oZGZakqSMjEz+LoshWXWknuZQU7Oop3nU1KzCqGde9uXRgHH8+HFlZGQoLCzMqT0sLEw7duxwu83atWv17rvvKi4uLlf3MW7cOI0dO9alffny5QoICMhzn93544iXJC/t3rVLS87uNLJPXBAbG+vpLlxzqKlZ1NM8amqWyXomJyfnel2PBoy8On36tHr37q2ZM2cqJCQkV9uMGjVK0dHRjttJSUkKDw9Xhw4dFBQUZKRfqxb8LB1LUPUaNdSlVVUj+7zepaWlKTY2Vu3bt5ePj4+nu3NNoKZmUU/zqKlZhVHPrLMAueHRgBESEiJvb28lJiY6tScmJqps2bIu6+/du1cHDhxQt27dHG2ZmZmSpGLFimnnzp2qWtX5AG+3253++msWHx8fYwX38rJJkry9vXhTGGbyecIF1NQs6mkeNTXLZD3zsh+PTvL09fVVZGSkVq5c6WjLzMzUypUrFRUV5bJ+rVq19MsvvyguLs7x74477lCbNm0UFxen8PDwK9l9AACQDY+fIomOjlbfvn3VpEkTNW3aVFOmTNHZs2fVv39/SVKfPn1UoUIFjRs3Tn5+fqpXr57T9sHBwZLk0g4AADzH4wGjZ8+eOnbsmMaMGaOEhAQ1bNhQS5cudUz8jI+Pl5eXx6+mBQAAeeDxgCFJw4cP1/Dhw90uW7NmTY7bzp4923yHAABAgTA0AAAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjCBgAAMA4AgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjCBgAAMA4AgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjCBgAAMA4AgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjCBgAAMA4AgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjCBgAAMA4AgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjCBgAAMA4AgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjLsqAsbUqVMVEREhPz8/NWvWTBs2bMh23ZkzZ6ply5YqVaqUSpUqpXbt2uW4PgAAuPI8HjDmzZun6OhoxcTEaPPmzWrQoIE6duyoo0ePul1/zZo1uv/++7V69WqtX79e4eHh6tChgw4fPnyFew4AALLj8YAxefJkDRo0SP3791edOnU0Y8YMBQQE6L333nO7/pw5czR06FA1bNhQtWrV0n//+19lZmZq5cqVV7jnAAAgO8U8eeepqanatGmTRo0a5Wjz8vJSu3bttH79+lztIzk5WWlpaSpdurTb5SkpKUpJSXHcTkpKkiSlpaUpLS2tAL3/W2amJUnKyMg0ts/rXVYdqac51NQs6mkeNTWrMOqZl315NGAcP35cGRkZCgsLc2oPCwvTjh07crWPp59+WuXLl1e7du3cLh83bpzGjh3r0r58+XIFBATkvdNu/HHES5KXdu/apSVndxrZJy6IjY31dBeuOdTULOppHjU1y2Q9k5OTc72uRwNGQY0fP15z587VmjVr5Ofn53adUaNGKTo62nE7KSnJMW8jKCjISD9WLfhZOpag6jVqqEurqkb2eb1LS0tTbGys2rdvLx8fH09355pATc2inuZRU7MKo55ZZwFyw6MBIyQkRN7e3kpMTHRqT0xMVNmyZXPcduLEiRo/frxWrFihm266Kdv17Ha77Ha7S7uPj4+xgnt52SRJ3t5evCkMM/k84QJqahb1NI+ammWynnnZj0cnefr6+ioyMtJpgmbWhM2oqKhst3v11Vf14osvaunSpWrSpMmV6CoAAMgDj58iiY6OVt++fdWkSRM1bdpUU6ZM0dmzZ9W/f39JUp8+fVShQgWNGzdOkjRhwgSNGTNGH330kSIiIpSQkCBJCgwMVGBgoMceBwAA+JvHA0bPnj117NgxjRkzRgkJCWrYsKGWLl3qmPgZHx8vL6+/B1qmT5+u1NRU3XPPPU77iYmJ0fPPP38luw4AALLh8YAhScOHD9fw4cPdLluzZo3T7QMHDhR+hwAAQIF4/Ie2AADAtYeAAQAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjCBgAAMA4AgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjCBgAAMA4AgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjCBgAAMA4AgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjCBgAAMA4AgYAADCOgAEAAIwjYADXseEfbtJNzy/V8A83uV2+4Kd4DZi9QQt+ir/CPUNRsnJ7gp799Get3J7gsmzArA2qM/prDZi1wQM9gycV83QHUDi2HvpLGw78qaYRpdUgvJSnuwMPmvntXi355Yg61Q1V2Yvabxy5WNb///+rXxO0eORi7R/f1bH81ldXKf7Pc5KklTuO6c1Ve/TtU20dy+/4z3f67UiS6pYL0hePtHS6z3umrdPW30+qQcVgLRzaorAeGtw4cuqc9h8/qxtDiqtcSX+nZQNmbdD6fScUVeUGvdu/qcu2K7cnaNWOo2pbK1S31S7rtCzrddSlfjkNurWqo/2uaeu0Of6kJGnOj4fUuFKwFv3/cx4xcvHf+955TBEjF+vARa8xSWr4/FKdPJ+hYD9vbXz2tlz3Z+znv2rZtkR1rBOmmDvruTyWms8uVkqGZPeWdr7c1WU5Ch8B4xr0+Pw4fbL5sOP23Y0raFKPhp7rEApswU/xWvprgjrVK6t7m1RyWe7uoJKanqmbX47VqXPpkqQth07J1+aldam/aflviY5wkcWSdN/b3+uuyHB9/csRR7jIEv/nOS34KV5dbyqvOmOWOdp//iPJ6cBx8UHlp/iTbg8qKJjsQsS8jfEa+ckvsiTZJI2/u7563nzh9XK5g727oLBwSHOdPp+uW19b5fQ6emPlbsV0q6sN+044tsmyOf6kHpz5g7YdSXLb9/7v/ajX72ukv5LT1GbiGkf7yfMZqj56ud6Iyr4/WcGlxrNLlJpx4RU8a/1BzdkQr68fu1Unk9N06lyq/jX7J8d+UzLk8lhz+gKWU3BG3tgsy7r0c+aalpSUpJIlS+rUqVMKCgoyss8Rczfr07gjeqpjdQ1tU8PIPvNr66G/dOfU713aPx/WvNBHMrL7hiPl/K3KnbS0NC1ZskRdunSRj49PYXX5qpJdjS4eSZCkSqX9HSMJlmXpv9/t1ytfb5dlXTio1CxbQufTMnTgRPIV7b/dW0rNkEtwkaQmla69kYzcvEaHf7hJ3+45plurldFbD0Y6LfvHy7FKOJ2qsiV89cOz7Z2W5fR+mbcxXqMW/aJMS/KyScPaVFPjyqW07+gZvbh4u0sfOtUrqx/2HtfJ/w8IF6sZGqjujStqa/xfWrotMa8lKBTFlKGuDSro862up1tKB/jqVHKqMvKx36yRjJy+gF0cwrJcHEwmLduhr39NUOd6ZfV4x1ou6168/aWh+uYXl+vY2TSVKe6jjaM7OC273MhSqwkrdfCv86pcyk/fPO08ypPTSE7Hyau18+hp1QwtoWXRbVz2mx95OYYSMAy4WgJGWkam7nzrO207csZlWc+bK2rC3Q0K7b4bv7hcf55Nc9wuXdxHm///TZTTt6rs5DZgFORUUH637fXOev108C81qVxKcwZH5ek+szvgXHrQeL5bXTWqVEof/XhAH2/83WU/jSqWVIakvUfP6Gxq3j5uw4J8lZiUmu3yltVD9N3u43naZ25kfeDmNWxe7LaJq7XveLKqhARo5RPOH5gFOS2T04GjxbgVOnwqRRVK2rVuVDtHe9ZrtFGLtvr9VIrL47n4FJR04bW/dmRbHT+dojunrnPpw56XO+vP5FR99EO83li12xEYW9cso7AgPx0/k6qEU+f06x/uRwautCC7t1IyMpTimlsU6OulM6mZV7xP4aX9deiSkbeL9W8eoVnfH3BpbxZRSlsP/6Xzaa7b3FQ+SJ//+xbVjVmm5Ivea/4+Xpoz6B9KOpem0+fT9e+Pt7hs625ULy/LLrf84pEcSfL1tmnXy11ytd/8ImDk4FoKGBefn6x8Q6Ci58fp599PuV23caWSWjT0lgLf56UH5eTUdI36ZKvbbxvPdqml2xuUV9S4VS7L1o9q6/RhfOnox8UBY1vCGbdBoCCngi63bXYHnNy8abM7neHugPPjM7fpx/0n9MjHcW6/+efXc11rK+lcqv6zau8lSyyN6lRD3+46oXX7/nTZrkWV0pozOEr1xizRmVTXHvn7SDbZlJzmuszufWE4OjsHxnfVvI3xevqTXxxtE9yEzeyCX0E+pKXsn5e6Y5Y6hbTivt5a+XhrHTudom5vrXXZ7w+jbtPxMylKOHlWM5du0oZjXo7nrlpooIp52bQ38bTclKjQVC4doISkc0pJd73TYP9ibkcvstzVuIJOn0tT7PajLssm3nuTjp8+r/FLd7kse7ZLLSWdS9Obqy99jUn/blNVH6w/oJPnXV8QQXYvbRzdQTWfW5pNjzIVVSVE6928Pp/pUks7jpzSoi1HXJb1j6qsmDvruX0tXI18vaVgf18dPeMa9ksHFFO1sCBt2v+n29GaQF+bQkv6a98x11HKYP9iOn0u3e12tcICtXREqwL1m4CRg2slYNw1ba02x/8dJmxyPzSdpWZYcS0b0bpA93npQblSaX8dP5PqlOovFujrJW8vL5067/rh1qlOmGb0aSLJ/ejHjyPbaMmSJfrmXLgWxf39YZIVBNbtPq5e7/7ost+LTwW5O1CdTUnXF3GHNerTX91uW79CsOrGLNW5tL+/ffl429S2VqhWb0+Uuy9lWQdlyfV0RrC/jzrVK6ulvxzRSTd1yEmQXzEln0+Xu60CfG2a3KOxgvyK6cF3f1TmRU++t82mtSPbqFxJf5faFi+WqbiYTvLx8cnxoLxye4IGvO96Zcm7fSP159lUPbnwF5dlr91T3217lqkPNNKwj1y/5a15orW8bDYdPX1ery3bqR/3/31gCS3hqyB/X+056joqJ0kBPpKyCTyNwoP05gOROnY6RYP+95OOX/RBHmj3Vquaodp04E8lJKVk2+crLbv38b2RFdWwUrC8ZNMzn/0iy83z/dEPB7M92G8++FeOgVJynvMgyWnOQ04jlO4C2m8vdJKUc/C7Z9o6/XTJ/A1JiiiertiRXdRz5oZs+5PXb+5ZSgX46K9k12GKyqX8dPCv89lulx1vm1SzbFC2802uNl6S9hVwFIOAkYOrLWDkZ7h4/sZ4PfWJ6wd5nbIl9GdyqtsPzIrBdq0d2c6l/VLZzaPYsP+Eerz9Q676lxv2YtKM3jdr+urd2nDgpMvykR2q6/De7fpgr+s85PBSfjqUzYdB9TLFFft4a5cwFF7KX3Yfb+09dkbZveJ9vKTMTOXr/G7X+uW08cAJHT2d/amH7FQM9tPvJ50fj5dNWjeybY4HjayRlXkb4/XMol+VYVnyttn0yl31nEYFZn67V0t/TVCHOmVU9tR2p9NOOZ3uyemAk928kKLy7bGgQkvY5VfMS/F/uQ7Hj+pcS+t2HdW3e10P6F3rhmlT/F9KcPM6KRPoo0+H3aJbX12dbWCUcn6+83uwz7Jye4LW7Dym1jXLuL2KJGsE6NI5VpOW7dCy3xLUsa7rKaaLrxKJe76T0zJ3fXojKt3xGs2pP2M//1Wx2xPVvrbz3IPbJq7W3uOu3+yrhgRoSOuq2YbjUQt/cRvmvST1a15Z731/0GVZ1vswv6/70gHF9Gey672WCfRRzB31NNxNIM/SKDxYWw6ddGlvX6uMdiacVvxJ189IRjAK2dUUMOZtjNfIRRe+jdhs0vi7Lj83YfWOoxr4/kZluHnWWtcI0Y6EJCW4Ob9eNshXPzxzYSJZdpd+ufuWMq1XpD7dfFifbvnd6RtDlv4tIjRn/QG33+w95ZZqIVq7x+wcgpDiPjp+1s0J2ly48YYAeXlJe90MZzaLCNa8IS3yfdDIcuTUOR04nqyIkIBsg2p+Js7m9AG/4Kd4Lf8tUR3qhjlOOeT3g9ZezEv+Pt46ec61xg82q6SPfoxXfl5ivsW8ZLMspbh57dYIDVRESICWb3M9NTCsdRV9tuWwDp9yDevlg+z6/pl2ij9+Wq0mfiNLNseyi8OAu1Ni+3NxSudygVHK+fnO6WBfkPlDheXiuTMfD2pqZHJ3TvXNadJ0Tttd7n2Y3bZjP/9Vs9a7hpP+UZXVvXGFHCflPzU/TvMv+qKUpUfjCnq1R0PmYFxtrpaAceTUOZe5CTZJ3180N+HiINCqRqgmxe7S9DWu32aztK4Rot/+SNIxN+f0ygT6auNz7bP9Vjpx6Q69lcO+s/P5sOYa/dmv+vmw6xBhgwpB2uqmPUuFYH8dPpn9hCw5poU6C7J7Kymnk/3ZuP/mcEV3qKmol1e4/aZik3R/s4r66EfXSZX/blPV7UhClme71NaRU+f03roDLsteu6e+bqle5rJzUfJ70MitK3FlTtaEyEtVCLIrNSNTx9yEtNIBxbRpdAf9/PvJbD9sV2xLzNe5/q3Pd9LCTYey/dZ6b5NK+f7Wn5aWptGzvtb8/d7KtOQ2DBTkKpLLBcZrkcnXaE4Tgt2F4yzVRi5Wui78hsOeSw7Il3sfZncVSU5B4HLzwaqOWuz0hdLbJu0d9/e+sxvJkbiK5Iq7WgLGB+sPaPTnv7m0v9S9rh78R4RLEAi0e+vM/x9U29Qso9U7j7ls+27fSD02N06n3Rx8S9i99XrPBhr4v80uy+qWD9JvOcxM79kkXHc1rqB5Px3SIjdvhJwuje393x/dhoESdi/9/Hwn1Xx2idvRj2KSosqk67tjrqdI+kdVdvuNIMuD/6ikD39w/eXJrG8FOX2jiLmzXrYHnOEfbtJXv7pOZr29XlnHwSOnb0e5meBYmK7Upb8FmYyZ04dtQYb/c3pepJwPHLm5iuTwqdTrLgwUhmv58vScgsDWQ3/ppwN/qUlEKbdXtD01P06rdh5V25qhejUPv2lUGPUkYOTgagkY9739vX7Y/5dLe7cGZdW9YQW3E+zsxWya1KOhbr+pfLYjEbWeWyx3cwmLSbJ5SWl5HGeuWiZAKx//O/lm90bI7sAw89u9ennJDpf9PtullgbdWjXbg33ff1RUY9sBPbnBx23yHzBrg1a6CVm31Syjd/s3vey3gpy+UUjZH3ByGvbOktO3I09+M72SH97ZHZSlnH8PQMr5wza/5/qlnJ+X/LiWD4aeQk3NImBcYVdDwDiZnKqbX4x1exnbP6qUUnqG9NNB1/DRLKKU5g1p7rjt7vx41s/j5oWvlzSiQw1NcHMpWtYwcm5kd2DIaQa65P5g/9vz7R1vjFeW7HSb/C/3rfVy3wpy+kaRk5yGva9mfHibRT3No6ZmeTpg8FPhV8DFl0veVDFYTy38Odtr5E8mp2lHgvtL8vx9vZ1u31a7rMvEu5ziYoVguw6fdD0/3iA8WA+3rq6PNxxyGUbOyze9BuHuD+SbR3fIcQb6rpe7uBzs09L+DiQxd9ZzGwAOjO+a4y/gZdefy+33copSqAAATyFgFLJLh+obhpdU3CH3P4YlKdtwIUm9oypf9v58inkp1c2khkBfL03rFel2rsRzt9eRJH37VFvjw8hZBt1a1SVYXCy/B3t3P6sLAPA8/lx7Idp66C+ncCHJES5K+mVf+gf/UUmNK5V0amtcKdhltMKdHpHhbtvvjQxXg/BSurtxBaf2uxtXcPqWf2+TSprZ92aj4QIAcP1hBKMQfbrF9fplSapYyk/pGZk6dd71ctISdi+91L2+pJx/gyA7MXfW05wN8S5zGrJGByb1aKg+UZVznJsAAEBBETAK0SY3EzUlqeoNgdp7/LTbZSX9/56I426ORW64m9NwscvNTQAAoKAIGIXEsiztTnD/2xKnU9NUMdhfv7uZcFmxVICR+8/vnAYAAExgDkYhWb3zqNz8uKAkKTU9U4NauZ/wOOjWKoXYKwAArgwCRiGwLEv/Wbkn2+UhgXbdVrusGlcKdmrP7UROAACudpwiKQTr9pxQ3KGTKmaT0t38LkXW5aaLhrbI10ROAACudgSMQvDmqt2SpAejIvTz7yddftL74iCR34mcAABczQgYhm3Y/6d+3P+nfL299FCrKipX0p9RCgDAdYeAYVjW6MU9TSo6/pgVoxQAgOsNkzwN+vn3JH23+7i8vWx6OJurRAAAuB4QMAxati1RkvTPRhUUXtrM71kAAFAUXRUBY+rUqYqIiJCfn5+aNWumDRs25Lj+ggULVKtWLfn5+al+/fpasmTJFeppzixLstmkoa0ZvQAAXN88HjDmzZun6OhoxcTEaPPmzWrQoIE6duyoo0ePul3/+++/1/33368BAwZoy5Yt6t69u7p3765ff/31Cvf8b5/GHXH8//abyqtKmUCP9QUAgKuBxwPG5MmTNWjQIPXv31916tTRjBkzFBAQoPfee8/t+m+88YY6deqkJ598UrVr19aLL76oxo0b66233rrCPb8gYuRip9tfbv3DI/0AAOBq4tGrSFJTU7Vp0yaNGjXK0ebl5aV27dpp/fr1brdZv369oqOjndo6duyozz77zO36KSkpSkn5+29+JCVd+PsgaWlpSktLK1D/645Z7ra9xqjF+u2FDgXa9/Uu67kp6HOEv1FTs6inedTUrMKoZ1725dGAcfz4cWVkZCgsLMypPSwsTDt27HC7TUJCgtv1ExIS3K4/btw4jR071qV9+fLlCggo2ETMVMsmydtNe8ZVMy+kqIuNjfV0F6451NQs6mkeNTXLZD2Tk5Nzve41/zsYo0aNchrxSEpKUnh4uDp06KCgoKAC7fvJH5Yr1c1PgfvavNWlCyMYBZGWlqbY2Fi1b99ePj4+l98Al0VNzaKe5lFTswqjnllnAXLDowEjJCRE3t7eSkxMdGpPTExU2bLuf5iqbNmyeVrfbrfLbre7tPv4+BS44LvGdXWZg5HVDjNMPE9wRk3Nop7mUVOzTNYzL/vx6CRPX19fRUZGauXKlY62zMxMrVy5UlFRUW63iYqKclpfujD8k936he3A+K7ytUlShnxtF24DAHC98/gpkujoaPXt21dNmjRR06ZNNWXKFJ09e1b9+/eXJPXp00cVKlTQuHHjJEmPPvqoWrVqpUmTJqlr166aO3eufvrpJ73zzjseewy/vdBBS5Ys4bQIAAD/z+MBo2fPnjp27JjGjBmjhIQENWzYUEuXLnVM5IyPj5eX198DLc2bN9dHH32k5557Ts8884yqV6+uzz77TPXq1fPUQwAAAJfweMCQpOHDh2v48OFul61Zs8al7d5779W9995byL0CAAD55fEf2gIAANceAgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjCBgAAMC4q+KvqV5JlmVJkpKSkoztMy0tTcnJyUpKSpKPj4+x/V7PqKl51NQs6mkeNTWrMOqZdezMOpbm5LoLGKdPn5YkhYeHe7gnAAAUTadPn1bJkiVzXMdm5SaGXEMyMzP1xx9/qESJErLZbEb2mZSUpPDwcB06dEhBQUFG9nm9o6bmUVOzqKd51NSswqinZVk6ffq0ypcvLy+vnGdZXHcjGF5eXqpYsWKh7DsoKIg3hWHU1Dxqahb1NI+ammW6npcbucjCJE8AAGAcAQMAABhHwDDAbrcrJiZGdrvd0125ZlBT86ipWdTTPGpqlqfred1N8gQAAIWPEQwAAGAcAQMAABhHwAAAAMYRMAAAgHEEjFyaOnWqIiIi5Ofnp2bNmmnDhg05rr9gwQLVqlVLfn5+ql+/vpYsWXKFelp05KWmM2fOVMuWLVWqVCmVKlVK7dq1u+xzcL3J62s0y9y5c2Wz2dS9e/fC7WARlNeanjx5UsOGDVO5cuVkt9tVo0YN3vsXyWs9p0yZopo1a8rf31/h4eEaMWKEzp8/f4V6e/X79ttv1a1bN5UvX142m02fffbZZbdZs2aNGjduLLvdrmrVqmn27NmF10ELlzV37lzL19fXeu+996zffvvNGjRokBUcHGwlJia6XX/dunWWt7e39eqrr1rbtm2znnvuOcvHx8f65ZdfrnDPr155rekDDzxgTZ061dqyZYu1fft2q1+/flbJkiWt33///Qr3/OqU13pm2b9/v1WhQgWrZcuW1p133nllOltE5LWmKSkpVpMmTawuXbpYa9eutfbv32+tWbPGiouLu8I9vzrltZ5z5syx7Ha7NWfOHGv//v3WsmXLrHLlylkjRoy4wj2/ei1ZssR69tlnrUWLFlmSrE8//TTH9fft22cFBARY0dHR1rZt26w333zT8vb2tpYuXVoo/SNg5ELTpk2tYcOGOW5nZGRY5cuXt8aNG+d2/R49elhdu3Z1amvWrJn10EMPFWo/i5K81vRS6enpVokSJaz333+/sLpYpOSnnunp6Vbz5s2t//73v1bfvn0JGJfIa02nT59uValSxUpNTb1SXSxS8lrPYcOGWW3btnVqi46Otlq0aFGo/SyqchMwnnrqKatu3bpObT179rQ6duxYKH3iFMllpKamatOmTWrXrp2jzcvLS+3atdP69evdbrN+/Xqn9SWpY8eO2a5/vclPTS+VnJystLQ0lS5durC6WWTkt54vvPCCQkNDNWDAgCvRzSIlPzX94osvFBUVpWHDhiksLEz16tXTK6+8ooyMjCvV7atWfurZvHlzbdq0yXEaZd++fVqyZIm6dOlyRfp8LbrSx6br7o+d5dXx48eVkZGhsLAwp/awsDDt2LHD7TYJCQlu109ISCi0fhYl+anppZ5++mmVL1/e5c1yPcpPPdeuXat3331XcXFxV6CHRU9+arpv3z6tWrVKvXr10pIlS7Rnzx4NHTpUaWlpiomJuRLdvmrlp54PPPCAjh8/rltuuUWWZSk9PV1DhgzRM888cyW6fE3K7tiUlJSkc+fOyd/f3+j9MYKBImf8+PGaO3euPv30U/n5+Xm6O0XO6dOn1bt3b82cOVMhISGe7s41IzMzU6GhoXrnnXcUGRmpnj176tlnn9WMGTM83bUiac2aNXrllVc0bdo0bd68WYsWLdLixYv14osverpryCVGMC4jJCRE3t7eSkxMdGpPTExU2bJl3W5TtmzZPK1/vclPTbNMnDhR48eP14oVK3TTTTcVZjeLjLzWc+/evTpw4IC6devmaMvMzJQkFStWTDt37lTVqlULt9NXufy8RsuVKycfHx95e3s72mrXrq2EhASlpqbK19e3UPt8NctPPUePHq3evXtr4MCBkqT69evr7NmzGjx4sJ599ll5efH9OK+yOzYFBQUZH72QGMG4LF9fX0VGRmrlypWOtszMTK1cuVJRUVFut4mKinJaX5JiY2OzXf96k5+aStKrr76qF198UUuXLlWTJk2uRFeLhLzWs1atWvrll18UFxfn+HfHHXeoTZs2iouLU3h4+JXs/lUpP6/RFi1aaM+ePY6wJkm7du1SuXLlrutwIeWvnsnJyS4hIiu8WfwJrXy54semQpk6eo2ZO3euZbfbrdmzZ1vbtm2zBg8ebAUHB1sJCQmWZVlW7969rZEjRzrWX7dunVWsWDFr4sSJ1vbt262YmBguU71EXms6fvx4y9fX11q4cKF15MgRx7/Tp0976iFcVfJaz0txFYmrvNY0Pj7eKlGihDV8+HBr586d1ldffWWFhoZaL730kqcewlUlr/WMiYmxSpQoYX388cfWvn37rOXLl1tVq1a1evTo4amHcNU5ffq0tWXLFmvLli2WJGvy5MnWli1brIMHD1qWZVkjR460evfu7Vg/6zLVJ5980tq+fbs1depULlO9Grz55ptWpUqVLF9fX6tp06bWDz/84FjWqlUrq2/fvk7rz58/36pRo4bl6+tr1a1b11q8ePEV7vHVLy81rVy5siXJ5V9MTMyV7/hVKq+v0YsRMNzLa02///57q1mzZpbdbreqVKlivfzyy1Z6evoV7vXVKy/1TEtLs55//nmratWqlp+fnxUeHm4NHTrU+uuvv658x69Sq1evdvu5mFXHvn37Wq1atXLZpmHDhpavr69VpUoVa9asWYXWP/5cOwAAMI45GAAAwDgCBgAAMI6AAQAAjCNgAAAA4wgYAADAOAIGAAAwjoABAACMI2AAAADjCBgArgk2m02fffaZJOnAgQOy2Wz8OXrAgwgYAAqsX79+stlsstls8vHx0Y033qinnnpK58+f93TXAHgIf64dgBGdOnXSrFmzlJaWpk2bNqlv376y2WyaMGGCp7sGwAMYwQBghN1uV9myZRUeHq7u3burXbt2io2NlXThT3OPGzdON954o/z9/dWgQQMtXLjQafvffvtNt99+u4KCglSiRAm1bNlSe/fulSRt3LhR7du3V0hIiEqWLKlWrVpp8+bNV/wxAsg9AgYA43799Vd9//338vX1lSSNGzdO//vf/zRjxgz99ttvGjFihB588EF98803kqTDhw/r1ltvld1u16pVq7Rp0yb961//Unp6uiTp9OnT6tu3r9auXasffvhB1atXV5cuXXT69GmPPUYAOeMUCQAjvvrqKwUGBio9PV0pKSny8vLSW2+9pZSUFL3yyitasWKFoqKiJElVqlTR2rVr9fbbb6tVq1aaOnWqSpYsqblz58rHx0eSVKNGDce+27Zt63Rf77zzjoKDg/XNN9/o9ttvv3IPEkCuETAAGNGmTRtNnz5dZ8+e1euvv65ixYrp7rvv1m+//abk5GS1b9/eaf3U1FQ1atRIkhQXF6eWLVs6wsWlEhMT9dxzz2nNmjU6evSoMjIylJycrPj4+EJ/XADyh4ABwIjixYurWrVqkqT33ntPDRo00Lvvvqt69epJkhYvXqwKFSo4bWO32yVJ/v7+Oe67b9++OnHihN544w1VrlxZdrtdUVFRSk1NLYRHAsAEAgYA47y8vPTMM88oOjpau3btkt1uV3x8vFq1auV2/Ztuuknvv/++0tLS3I5irFu3TtOmTVOXLl0kSYcOHdLx48cL9TEAKBgmeQIoFPfee6+8vb319ttv64knntCIESP0/vvva+/evdq8ebPefPNNvf/++5Kk4cOHKykpSffdd59++ukn7d69Wx988IF27twpSapevbo++OADbd++XT/++KN69ep12VEPAJ7FCAaAQlGsWDENHz5cr776qvbv368yZcpo3Lhx2rdvn4KDg9W4cWM988wzkqQbbrhBq1at0pNPPqlWrVrJ29tbDRs2VIsWLSRJ7777rgYPHqzGjRsrPDxcr7zyip544glPPjwAl2GzLMvydCcAAMC1hVMkAADAOAIGAAAwjoABAACMI2AAAADjCBgAAMA4AgYAADCOgAEAAIwjYAAAAOMIGAAAwDgCBgAAMI6AAQAAjPs/uFfgTkJXDRgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_pr_curve(gt, pred, class_name):\n",
    "    \"\"\"Plot the Precision-Recall curve for a specific class.\"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(gt.numpy(), pred.numpy())\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    plt.title(f'Precision-Recall Curve for {class_name}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot PR curve for a specific class, e.g., Cardiomegaly (index 1)\n",
    "plot_pr_curve(gt[:, 1], pred[:, 1], \"Cardiomegaly\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***4. Threshold Optimization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Thresholds:\n",
      "Emphysema: 0.00\n",
      "Cardiomegaly: 0.04\n",
      "Edema: 0.10\n",
      "Effusion: 0.26\n",
      "Atelectasis: 0.24\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def optimize_thresholds(gt, pred):\n",
    "    \"\"\"Find the optimal threshold for each class.\"\"\"\n",
    "    gt_np = gt.numpy()\n",
    "    pred_np = pred.numpy()\n",
    "    optimal_thresholds = []\n",
    "\n",
    "    for i in range(N_CLASSES):\n",
    "        best_threshold = 0.5\n",
    "        best_f1 = 0\n",
    "        for t in np.arange(0.0, 1.01, 0.01):\n",
    "            pred_class = (pred_np[:, i] > t).astype(int)\n",
    "            f1 = f1_score(gt_np[:, i], pred_class, zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = t\n",
    "        optimal_thresholds.append(best_threshold)\n",
    "\n",
    "    return optimal_thresholds\n",
    "\n",
    "# Find and print optimal thresholds\n",
    "optimal_thresholds = optimize_thresholds(gt, pred)\n",
    "print(\"\\nOptimal Thresholds:\")\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    print(f\"{class_name}: {optimal_thresholds[i]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(gt, pred, threshold=0.5):\n",
    "    \"\"\"Analyze false positives and false negatives for each class.\"\"\"\n",
    "    gt_np = gt.numpy()\n",
    "    pred_np = (pred.numpy() > threshold).astype(int)\n",
    "\n",
    "    for i, class_name in enumerate(CLASS_NAMES):\n",
    "        false_positives = np.sum((pred_np[:, i] == 1) & (gt_np[:, i] == 0))\n",
    "        false_negatives = np.sum((pred_np[:, i] == 0) & (gt_np[:, i] == 1))\n",
    "        print(f\"{class_name}: False Positives = {false_positives}, False Negatives = {false_negatives}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for dimension 1 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Plot calibration curve for a specific class, e.g., Pneumonia (index 6)\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m plot_calibration_curve(\u001b[43mgt\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m]\u001b[49m, pred[:, \u001b[38;5;241m6\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPneumonia\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 6 is out of bounds for dimension 1 with size 5"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "def plot_calibration_curve(gt, pred, class_name):\n",
    "    \"\"\"Plot the calibration curve for a specific class.\"\"\"\n",
    "    prob_true, prob_pred = calibration_curve(gt.numpy(), pred.numpy(), n_bins=10)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(prob_pred, prob_true, marker='.')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=1)  # Diagonal reference line\n",
    "    plt.title(f'Calibration Curve for {class_name}')\n",
    "    plt.xlabel('Mean Predicted Probability')\n",
    "    plt.ylabel('Fraction of Positives')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot calibration curve for a specific class, e.g., Pneumonia (index 6)\n",
    "plot_calibration_curve(gt[:, 6], pred[:, 6], \"Pneumonia\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify the Model for Grad-CAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet121(nn.Module):\n",
    "    \"\"\"DenseNet121 model for multi-label classification.\"\"\"\n",
    "    def __init__(self, out_size):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        self.densenet121 = densenet121(pretrained=False)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Linear(num_ftrs, out_size)\n",
    "        self.features = self.densenet121.features  # Access the convolutional features\n",
    "        self.classifier = self.densenet121.classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = nn.ReLU(inplace=True)(features)  # Activation\n",
    "        out = nn.AdaptiveAvgPool2d((1, 1))(out).view(features.size(0), -1)  # Global Pooling\n",
    "        out = self.classifier(out)\n",
    "        return out, features  # Return features for Grad-CAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "\n",
    "        # Hook the gradients and activations\n",
    "        self.target_layer.register_forward_hook(self._save_activations)\n",
    "        self.target_layer.register_backward_hook(self._save_gradients)\n",
    "\n",
    "    def _save_activations(self, module, input, output):\n",
    "        self.activations = output\n",
    "\n",
    "    def _save_gradients(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0]\n",
    "\n",
    "    def generate_heatmap(self, class_idx):\n",
    "        \"\"\"Generate Grad-CAM heatmap for the target class index.\"\"\"\n",
    "        weights = self.gradients.mean(dim=(2, 3), keepdim=True)  # Global average of gradients\n",
    "        heatmap = torch.sum(weights * self.activations, dim=1).squeeze(0)\n",
    "        heatmap = F.relu(heatmap)\n",
    "        heatmap /= torch.max(heatmap)  # Normalize to [0, 1]\n",
    "        return heatmap.cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def overlay_heatmap(image, heatmap, alpha=0.5, colormap=cv2.COLORMAP_JET):\n",
    "    \"\"\"Overlay the heatmap on the original image.\"\"\"\n",
    "    heatmap = cv2.applyColorMap((heatmap * 255).astype(np.uint8), colormap)\n",
    "    overlay = cv2.addWeighted(heatmap, alpha, image, 1 - alpha, 0)\n",
    "    return overlay\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Preprocess an image for the model.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    input_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image, input_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradcam_example_random(model, dataset, class_idx):\n",
    "    # Pick a random image with the specified class\n",
    "    raw_image, target = pick_random_image_with_class(dataset, class_idx)\n",
    "    print(f\"Randomly selected image has target labels: {target}\")\n",
    "\n",
    "    # Preprocess the image for the model\n",
    "    transform = create_transforms()  # Assuming you have the same transform function\n",
    "    input_tensor = transform(raw_image.unsqueeze(0))  # Add batch dimension\n",
    "\n",
    "    # Run the model and compute gradients\n",
    "    model.eval()\n",
    "    input_tensor = input_tensor.cuda()\n",
    "    model = model.cuda()\n",
    "    output, features = model(input_tensor)\n",
    "    loss = output[0, class_idx]\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Generate Grad-CAM heatmap\n",
    "    grad_cam = GradCAM(model, model.features[-1])\n",
    "    heatmap = grad_cam.generate_heatmap(class_idx)\n",
    "\n",
    "    # Overlay heatmap on the image\n",
    "    heatmap_resized = cv2.resize(heatmap, (raw_image.size(1), raw_image.size(2)))\n",
    "    heatmap_overlay = overlay_heatmap(np.array(raw_image.permute(1, 2, 0).cpu() * 255, dtype=np.uint8), heatmap_resized)\n",
    "\n",
    "    # Display the result\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(raw_image.permute(1, 2, 0).cpu())\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(heatmap_overlay)\n",
    "    plt.title(\"Grad-CAM Heatmap\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def pick_random_image_with_class(dataset, class_idx):\n",
    "    \"\"\"Pick a random image from the dataset with the specified class.\"\"\"\n",
    "    indices = []\n",
    "    for i in range(len(dataset)):\n",
    "        _, target = dataset[i]\n",
    "        if target[class_idx] == 1:  # Check if the class is positive\n",
    "            indices.append(i)\n",
    "\n",
    "    if len(indices) == 0:\n",
    "        raise ValueError(f\"No images found with class index {class_idx} as positive.\")\n",
    "\n",
    "    selected_idx = random.choice(indices)\n",
    "    image, target = dataset[selected_idx]\n",
    "    return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected image has target labels: tensor([1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [3, 224, 224] and output size of [256, 256]. Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Use the function with a random Cardiomegaly image\u001b[39;00m\n\u001b[0;32m      6\u001b[0m class_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Cardiomegaly index\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mgradcam_example_random\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_idx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 8\u001b[0m, in \u001b[0;36mgradcam_example_random\u001b[1;34m(model, dataset, class_idx)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Preprocess the image for the model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m transform \u001b[38;5;241m=\u001b[39m create_transforms()  \u001b[38;5;66;03m# Assuming you have the same transform function\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Run the model and compute gradients\u001b[39;00m\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\functional.py:479\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39mpil_interpolation)\n\u001b[1;32m--> 479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\_functional_tensor.py:467\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, antialias)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;66;03m# Define align_corners to avoid warnings\u001b[39;00m\n\u001b[0;32m    465\u001b[0m align_corners \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m out_dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39muint8:\n\u001b[0;32m    470\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:4453\u001b[0m, in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   4451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m   4452\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m-> 4453\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   4454\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput and output must have the same number of spatial dimensions, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4455\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput with spatial dimensions of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and output size of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4456\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide input tensor in (N, C, d1, d2, ...,dK) format and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4457\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput size in (o1, o2, ...,oK) format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4458\u001b[0m         )\n\u001b[0;32m   4459\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m   4460\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_integer(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m size):\n",
      "\u001b[1;31mValueError\u001b[0m: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [3, 224, 224] and output size of [256, 256]. Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format."
     ]
    }
   ],
   "source": [
    "# Assuming you have defined the `ChestXrayDataSet` class\n",
    "transform = create_transforms()\n",
    "test_dataset = ChestXrayDataSet(data_dir=DATA_DIR, image_list_file=TEST_IMAGE_LIST, transform=transform)\n",
    "\n",
    "# Use the function with a random Cardiomegaly image\n",
    "class_idx = 1  # Cardiomegaly index\n",
    "gradcam_example_random(model, test_dataset, class_idx)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
